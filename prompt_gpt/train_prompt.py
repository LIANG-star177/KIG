# -*- coding:utf-8 -*-
# @project: GPT2-NewsTitle
# @filename: train.py
# @author: åˆ˜èªNLP
# @contact: logcongcong@gmail.com
# @time: 2020/12/16 16:28
"""
    æ–‡ä»¶è¯´æ˜ï¼š
    é€šè¿‡æ–°é—»æ­£æ–‡ç”Ÿæˆæ–°é—»æ ‡é¢˜çš„GPT2æ¨¡å‹çš„è®­ç»ƒæ–‡ä»¶
"""

import torch
import os
import random
import numpy as np
import argparse
import logging
# from distill_tuning import Distill_Tuning
from dialog_tuning import Distill_Tuning
from transformers import BertTokenizer
from dataset import GPT2NewsTitleDataSet, collate_func
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import AdamW, get_linear_schedule_with_warmup, GPT2Config
from tqdm import tqdm, trange
try:
    from torch.utils.tensorboard import SummaryWriter
except ImportError:
    from tensorboardX import SummaryWriter


logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)


def train(model, device, train_data, test_data, args):
    tb_write = SummaryWriter()
    if args.gradient_accumulation_steps < 1:
        raise ValueError("gradient_accumulation_stepså‚æ•°æ— æ•ˆï¼Œå¿…é¡»å¤§äºç­‰äº1")
    # è®¡ç®—çœŸå®çš„è®­ç»ƒbatch_sizeå¤§å°
    train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)
    train_sampler = RandomSampler(train_data)
    train_data_loader = DataLoader(train_data, sampler=train_sampler,
                                   batch_size=train_batch_size, collate_fn=collate_func)
    total_steps = int(len(train_data_loader) * args.num_train_epochs / args.gradient_accumulation_steps)
    logger.info("æ€»è®­ç»ƒæ­¥æ•°ä¸º:{}".format(total_steps))
    model.to(device)
    # è·å–æ¨¡å‹æ‰€æœ‰å‚æ•°
    params = [{'params': model.prompt_encoder.parameters(), 'lr':args.prompt_lr}]
    if args.use_lm_finetune:
        params.append({'params': model.model.parameters(), 'lr': args.finetune_lr})

    optimizer = AdamW(params, eps=args.adam_epsilon, weight_decay= 0.01)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(args.warmup_proportion * total_steps),
                                                num_training_steps=total_steps)
    # æ¸…ç©ºcudaç¼“å­˜
    torch.cuda.empty_cache()
    # å°†æ¨¡å‹è°ƒè‡³è®­ç»ƒçŠ¶æ€
    model.train()
    title_id = train_data.title_id
    tr_loss, logging_loss, min_loss = 0.0, 0.0, 0.0
    global_step = 0
    # å¼€å§‹è®­ç»ƒæ¨¡å‹
    for iepoch in trange(0, int(args.num_train_epochs), desc="Epoch", disable=False):
        iter_bar = tqdm(train_data_loader, desc="Iter (loss=X.XXX)", disable=False)
        for step, batch in enumerate(iter_bar):
            input_ids = batch["input_ids"].to(device)
            token_type_ids = batch["token_type_ids"].to(device)
            claim_label2 = batch["claim_label2"].to(device)
            # è·å–è®­ç»ƒç»“æœ
            outputs = model.forward(input_ids=input_ids, token_type_ids=token_type_ids, labels=input_ids,\
                                    title_id=title_id, claim_label2 = claim_label2)
            loss = outputs[0]
            tr_loss += loss.item()
            # å°†æŸå¤±å€¼æ”¾åˆ°Iterä¸­ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
            iter_bar.set_description("Iter (loss=%5.3f)" % loss.item())
            # åˆ¤æ–­æ˜¯å¦è¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œå¦‚æœè¿›è¡Œï¼Œåˆ™å°†æŸå¤±å€¼é™¤ä»¥ç´¯ç§¯æ­¥æ•°
            if args.gradient_accumulation_steps > 1:
                loss = loss / args.gradient_accumulation_steps
            # æŸå¤±è¿›è¡Œå›ä¼ 
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
            # å½“è®­ç»ƒæ­¥æ•°æ•´é™¤ç´¯ç§¯æ­¥æ•°æ—¶ï¼Œè¿›è¡Œå‚æ•°ä¼˜åŒ–
            if (step + 1) % args.gradient_accumulation_steps == 0:
                torch.cuda.empty_cache()
                optimizer.step()
                scheduler.step()
                torch.cuda.empty_cache()
                optimizer.zero_grad()
                global_step += 1
                # å¦‚æœæ­¥æ•°æ•´é™¤logging_stepsï¼Œåˆ™è®°å½•å­¦ä¹ ç‡å’Œè®­ç»ƒé›†æŸå¤±å€¼
                if args.logging_steps > 0 and global_step % args.logging_steps == 0:
                    tb_write.add_scalar("lr", scheduler.get_lr()[0], global_step)
                    tb_write.add_scalar("train_loss", (tr_loss-logging_loss) /
                                        (args.logging_steps*args.gradient_accumulation_steps), global_step)
                    logging_loss = tr_loss
                # å¦‚æœæ­¥æ•°æ•´é™¤eval_stepsï¼Œåˆ™è¿›è¡Œæ¨¡å‹æµ‹è¯•ï¼Œè®°å½•æµ‹è¯•é›†çš„æŸå¤±
                if args.eval_steps > 0 and global_step % args.eval_steps == 0:
                    eval_loss = evaluate(model, device, test_data, args)
                    tb_write.add_scalar("test_loss", eval_loss, global_step)
                    model.train()
        # æ¯ä¸ªepochè¿›è¡Œå®Œï¼Œåˆ™ä¿å­˜æ¨¡å‹
        prompt_output_dir = os.path.join(args.prompt_output_dir, "checkpoint-{}".format(global_step))
        torch.save(model.prompt_encoder.state_dict(),prompt_output_dir)
        if args.use_lm_finetune:
            output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
            model.model.save_pretrained(output_dir)
        # model_to_save = model.module if hasattr(model, "module") else model
        # model_to_save.save_pretrained(output_dir)
        # æ¸…ç©ºcudaç¼“å­˜
        # torch.cuda.empty_cache()

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def evaluate(model, device, test_data, args):
    test_sampler = SequentialSampler(test_data)
    test_data_loader = DataLoader(test_data, sampler=test_sampler,
                                  batch_size=args.test_batch_size, collate_fn=collate_func)
    iter_bar = tqdm(test_data_loader, desc="iter", disable=False)
    title_id = test_data.title_id
    total_loss, total = 0.0, 0.0
    # è¿›è¡Œæµ‹è¯•
    for step, batch in enumerate(iter_bar):
        # æ¨¡å‹è®¾ä¸ºeval
        model.eval()
        with torch.no_grad():
            input_ids = batch["input_ids"].to(device)
            token_type_ids = batch["token_type_ids"].to(device)
            claim_label2 = batch["claim_label2"].to(device)
            # è·å–é¢„æµ‹ç»“æœ
            outputs = model.forward(input_ids=input_ids, token_type_ids=token_type_ids, labels=input_ids,\
                                    title_id=title_id, claim_label2 = claim_label2)
            loss = outputs[0]
            loss = loss.item()
            # å¯¹lossè¿›è¡Œç´¯åŠ 
            total_loss += loss*len(batch["input_ids"])
            total += len(batch["input_ids"])
    # è®¡ç®—æœ€ç»ˆæµ‹è¯•é›†çš„lossç»“æœ
    test_loss = total_loss / total
    return test_loss


def set_args():
    """è®¾ç½®è®­ç»ƒæ¨¡å‹æ‰€éœ€å‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', default='0', type=str, help='è®¾ç½®è®­ç»ƒæˆ–æµ‹è¯•æ—¶ä½¿ç”¨çš„æ˜¾å¡')
    parser.add_argument('--config_path', default='new_gpt/config.json', type=str, help='æ¨¡å‹å‚æ•°é…ç½®ä¿¡æ¯')
    parser.add_argument('--vocab_path', default='new_gpt/vocab.txt', type=str, help='è¯è¡¨ï¼Œè¯¥è¯è¡¨ä¸ºå°è¯è¡¨ï¼Œå¹¶å¢åŠ äº†ä¸€äº›æ–°çš„æ ‡è®°')
    parser.add_argument('--train_file_path', default='data/new_split/train.json', type=str, help='æ–°é—»æ ‡é¢˜ç”Ÿæˆçš„è®­ç»ƒæ•°æ®')
    parser.add_argument('--test_file_path', default='data/new_split/test.json', type=str, help='æ–°é—»æ ‡é¢˜ç”Ÿæˆçš„æµ‹è¯•æ•°æ®')
    parser.add_argument('--pretrained_model_path', default="log/checkpoint-41680", type=str, help='é¢„è®­ç»ƒçš„GPT2æ¨¡å‹çš„è·¯å¾„')
    parser.add_argument('--data_dir', default='data/new_split', type=str, help='ç”Ÿæˆç¼“å­˜æ•°æ®çš„å­˜æ”¾è·¯å¾„')
    parser.add_argument('--num_train_epochs', default=10, type=int, help='æ¨¡å‹è®­ç»ƒçš„è½®æ•°')
    parser.add_argument('--train_batch_size', default=16, type=int, help='è®­ç»ƒæ—¶æ¯ä¸ªbatchçš„å¤§å°')
    parser.add_argument('--test_batch_size', default=8, type=int, help='æµ‹è¯•æ—¶æ¯ä¸ªbatchçš„å¤§å°')
    parser.add_argument('--warmup_proportion', default=0.1, type=float, help='warm upæ¦‚ç‡ï¼Œå³è®­ç»ƒæ€»æ­¥é•¿çš„ç™¾åˆ†ä¹‹å¤šå°‘ï¼Œè¿›è¡Œwarm up')
    parser.add_argument('--adam_epsilon', default=1e-8, type=float, help='Adamä¼˜åŒ–å™¨çš„epsilonå€¼')
    parser.add_argument('--logging_steps', default=20, type=int, help='ä¿å­˜è®­ç»ƒæ—¥å¿—çš„æ­¥æ•°')
    parser.add_argument('--eval_steps', default=1000, type=int, help='è®­ç»ƒæ—¶ï¼Œå¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡æµ‹è¯•')
    parser.add_argument('--gradient_accumulation_steps', default=4, type=int, help='æ¢¯åº¦ç§¯ç´¯')
    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='')
    parser.add_argument('--seed', type=int, default=2022, help='éšæœºç§å­')
    parser.add_argument('--max_len', type=int, default=900, help='è¾“å…¥æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œè¦æ¯”configä¸­n_ctxå°')
    parser.add_argument('--title_max_len', type=int, default=300, help='ç”Ÿæˆæ ‡é¢˜çš„æœ€å¤§é•¿åº¦ï¼Œè¦æ¯”max_lenå°')

    #prompt args
    parser.add_argument("--multi_prompt", type=bool, default=True, help='æ˜¯å¦å¤šç»´prompt')
    parser.add_argument("--context_aware", type=bool, default=True, help='æ˜¯å¦æ³¨æ„ä¸Šä¸‹æ–‡')
    parser.add_argument("--use_lm_finetune", type=bool, default=False, help='æ˜¯å¦finetune')
    parser.add_argument('--prompt_lr', default=1e-4, type=float, required=False, help='å­¦ä¹ ç‡')
    parser.add_argument('--finetune_lr', default=1e-5, type=float, required=False, help='å­¦ä¹ ç‡')
    parser.add_argument('--template_len', default=5, type=int, required=False,help='prompté•¿åº¦')
    parser.add_argument("--pseudo_token", type=str, default='##ğŸ”¥')
    parser.add_argument("--lstm_dropout", type=float, default=0.0)
    parser.add_argument('--output_dir', default='log/ca_p_finetune_saved_model', type=str, help='æ¨¡å‹è¾“å‡ºè·¯å¾„')
    parser.add_argument('--prompt_output_dir', default='log/ca_multi_mean_prompt_saved_model', type=str, help='æ¨¡å‹è¾“å‡ºè·¯å¾„')
    return parser.parse_args()


def main():
    # è®¾ç½®æ¨¡å‹è®­ç»ƒå‚æ•°
    args = set_args()
    # è®¾ç½®æ˜¾å¡ä¿¡æ¯
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICE"] = args.device
    # è·å–deviceä¿¡æ¯ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ
    device = torch.device("cuda:0" if torch.cuda.is_available() and int(args.device) >= 0 else "cpu")
    args.device = device
    # è®¾ç½®éšæœºç§å­ï¼Œæ–¹ä¾¿æ¨¡å‹å¤ç°
    if args.seed:
        torch.manual_seed(args.seed)
        random.seed(args.seed)
        np.random.seed(args.seed)
    # åŠ è½½æ¨¡å‹çš„config
    model = Distill_Tuning(args)
    print(count_parameters(model))
    tokenizer = BertTokenizer.from_pretrained(args.vocab_path, do_lower_case=True)
    if not os.path.exists(args.output_dir):
        os.mkdir(args.output_dir)
    if not os.path.exists(args.prompt_output_dir):
        os.mkdir(args.prompt_output_dir)
    train_data = GPT2NewsTitleDataSet(tokenizer, args.max_len, args.title_max_len, args.data_dir, "train", args.train_file_path)
    test_data = GPT2NewsTitleDataSet(tokenizer, args.max_len, args.title_max_len, args.data_dir, "test", args.test_file_path)
    # å¼€å§‹è®­ç»ƒ
    train(model, device, train_data, test_data, args)


if __name__ == '__main__':
    main()

