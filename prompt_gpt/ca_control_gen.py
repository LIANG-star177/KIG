# -*- coding:utf-8 -*-
# @project: GPT2-NewsTitle
# @filename: generate_title.py
# @author: åˆ˜èªNLP
# @contact: logcongcong@gmail.com
# @time: 2020/12/16 16:29
"""
    æ–‡ä»¶è¯´æ˜ï¼š
    æ ¹æ®è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œæ–°é—»æ ‡é¢˜ç”Ÿæˆï¼Œé¢„æµ‹æ–‡ä»¶
"""
from torch.utils.data import DataLoader, Dataset
import torch
import os
import argparse
from transformers import BertTokenizer
import torch.nn.functional as F
import copy
import sys 
import random
import numpy as np  
sys.path.append("disc") 
import pandas as pd
import json
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from dialog_tuning import Distill_Tuning
from disc_judge import cal_ans
from Transformer_sent_model import Transformer
from mytokenizer import MyTokenizer
from prompt_head import CaPromptHead


def set_args():
    """è®¾ç½®æ¨¡å‹é¢„æµ‹æ‰€éœ€å‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', default='0', type=str, help='è®¾ç½®é¢„æµ‹æ—¶ä½¿ç”¨çš„æ˜¾å¡,ä½¿ç”¨CPUè®¾ç½®æˆ-1å³å¯')
    parser.add_argument('--pretrained_model_path', default="log/checkpoint-41680", type=str, help='é¢„è®­ç»ƒçš„GPT2æ¨¡å‹çš„è·¯å¾„')
    parser.add_argument('--prompt_model_path', default='log/ca_multi_mean_prompt_saved_model/checkpoint-20840', type=str, help='æ¨¡å‹è¾“å‡ºè·¯å¾„')
    parser.add_argument('--vocab_path', default='new_gpt/vocab.txt', type=str, help='è¯è¡¨ï¼Œè¯¥è¯è¡¨ä¸ºå°è¯è¡¨ï¼Œå¹¶å¢åŠ äº†ä¸€äº›æ–°çš„æ ‡è®°')
    parser.add_argument('--batch_size', default=1, type=int, help='ç”Ÿæˆæ ‡é¢˜çš„ä¸ªæ•°')
    parser.add_argument('--generate_max_len', default=300, type=int, help='ç”Ÿæˆæ ‡é¢˜çš„æœ€å¤§é•¿åº¦')
    parser.add_argument('--repetition_penalty', default=1.2, type=float, help='é‡å¤å¤„ç½šç‡')
    parser.add_argument('--top_k', default=5, type=float, help='è§£ç æ—¶ä¿ç•™æ¦‚ç‡æœ€é«˜çš„å¤šå°‘ä¸ªæ ‡è®°')
    parser.add_argument('--top_p', default=0.95, type=float, help='è§£ç æ—¶ä¿ç•™æ¦‚ç‡ç´¯åŠ å¤§äºå¤šå°‘çš„æ ‡è®°')
    parser.add_argument('--max_len', type=int, default=900, help='è¾“å…¥æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œè¦æ¯”configä¸­n_ctxå°')
    parser.add_argument('--test_file_path', default='data/new_split/test.json', type=str, help='æ–°é—»æ ‡é¢˜ç”Ÿæˆçš„æµ‹è¯•æ•°æ®')
    parser.add_argument('--test_num', default=500, type=int, help='æµ‹è¯•æ•°é‡')

    #åˆ¤åˆ«å™¨è®¾ç½®
    parser.add_argument('--disc_model_path', default='model/disc_model/model_9', type=str)
    parser.add_argument('--embedding_path', default='model/word2vec/word2vec.model', type=str)
    parser.add_argument('--claim_map_path', default="data/claim_l2i_multi.json", type=str)

    # promptå‚æ•°
    parser.add_argument("--multi_prompt", type=bool, default=True, help='æ˜¯å¦å¤šç»´prompt')
    parser.add_argument("--context_aware", type=bool, default=True, help='æ˜¯å¦æ³¨æ„ä¸Šä¸‹æ–‡')
    parser.add_argument("--use_lm_finetune", type=bool, default=False, help='æ˜¯å¦finetune')
    parser.add_argument('--template_len', default=5, type=int, required=False,help='prompté•¿åº¦')
    parser.add_argument("--pseudo_token", type=str, default='##ğŸ”¥')
    parser.add_argument("--lstm_dropout", type=float, default=0.0)
 
    # ç»“æœè¾“å‡º
    parser.add_argument('--ans_filename', default="log/pred_ans2.txt", type=str)
    parser.add_argument('--txt_filename', default="log/pred2.txt", type=str)
    parser.add_argument('--metric_filename', default="log/pred_metric2.txt", type=str)
    return parser.parse_args()


def top_k_top_p_filtering(logits, top_k, top_p, filter_value=-float("Inf")):
    """
    top_kæˆ–top_pè§£ç ç­–ç•¥ï¼Œä»…ä¿ç•™top_kä¸ªæˆ–ç´¯ç§¯æ¦‚ç‡åˆ°è¾¾top_pçš„æ ‡è®°ï¼Œå…¶ä»–æ ‡è®°è®¾ä¸ºfilter_valueï¼Œåç»­åœ¨é€‰å–æ ‡è®°çš„è¿‡ç¨‹ä¸­ä¼šå–ä¸åˆ°å€¼è®¾ä¸ºæ— ç©·å°ã€‚
    Args:
        logits: é¢„æµ‹ç»“æœï¼Œå³é¢„æµ‹æˆä¸ºè¯å…¸ä¸­æ¯ä¸ªè¯çš„åˆ†æ•°
        top_k: åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„top_kä¸ªæ ‡è®°
        top_p: åªä¿ç•™æ¦‚ç‡ç´¯ç§¯è¾¾åˆ°top_pçš„æ ‡è®°
        filter_value: è¿‡æ»¤æ ‡è®°å€¼

    Returns:

    """
    # logitsçš„ç»´åº¦å¿…é¡»ä¸º2ï¼Œå³size:[batch_size, vocab_size]
    assert logits.dim() == 2
    # è·å–top_kå’Œå­—å…¸å¤§å°ä¸­è¾ƒå°çš„ä¸€ä¸ªï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœtop_kå¤§äºå­—å…¸å¤§å°ï¼Œåˆ™å–å­—å…¸å¤§å°ä¸ªæ ‡è®°
    top_k = min(top_k, logits[0].size(-1))
    # å¦‚æœtop_kä¸ä¸º0ï¼Œåˆ™å°†åœ¨logitsä¸­ä¿ç•™top_kä¸ªæ ‡è®°
    if top_k > 0:
        # ç”±äºæœ‰batch_sizeä¸ªé¢„æµ‹ç»“æœï¼Œå› æ­¤å¯¹å…¶éå†ï¼Œé€‰å–æ¯ä¸ªé¢„æµ‹ç»“æœçš„top_kæ ‡è®°
        for logit in logits:
            indices_to_remove = logit < torch.topk(logit, top_k)[0][..., -1, None]
            logit[indices_to_remove] = filter_value
    # å¦‚æœtop_pä¸ä¸º0ï¼Œåˆ™å°†åœ¨logitsä¸­ä¿ç•™æ¦‚ç‡å€¼ç´¯ç§¯è¾¾åˆ°top_pçš„æ ‡è®°
    if top_p > 0.0:
        # å¯¹logitsè¿›è¡Œé€’å‡æ’åº
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        # å¯¹æ’åºåçš„ç»“æœä½¿ç”¨softmaxå½’ä¸€åŒ–ï¼Œå†è·å–ç´¯ç§¯æ¦‚ç‡åºåˆ—
        # ä¾‹å¦‚ï¼šåŸå§‹åºåˆ—[0.1, 0.2, 0.3, 0.4]ï¼Œåˆ™å˜ä¸ºï¼š[0.1, 0.3, 0.6, 1.0]
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        # åˆ é™¤ç´¯ç§¯æ¦‚ç‡é«˜äºtop_pçš„æ ‡è®°
        sorted_indices_to_remove = cumulative_probs > top_p
        # å°†ç´¢å¼•å‘å³ç§»åŠ¨ï¼Œä½¿ç¬¬ä¸€ä¸ªæ ‡è®°ä¹Ÿä¿æŒåœ¨top_pä¹‹ä¸Š
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        for index, logit in enumerate(logits):
            # ç”±äºæœ‰batch_sizeä¸ªé¢„æµ‹ç»“æœï¼Œå› æ­¤å¯¹å…¶éå†ï¼Œé€‰å–æ¯ä¸ªé¢„æµ‹ç»“æœçš„ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°top_pçš„æ ‡è®°
            indices_to_remove = sorted_indices[index][sorted_indices_to_remove[index]]
            logit[indices_to_remove] = filter_value
    return logits

def one_hot_labels(labels_index, arg_map):
    label=[0]*len(arg_map)
    for item in labels_index:
        label[int(item)] = 1
    return label

def predict_one_sample(model, prompt_head, tokenizer, device, args, content, claim_label2 = None):
    """
    å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
    Args:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡ä¿¡æ¯
        args: é…ç½®é¡¹ä¿¡æ¯
        content: æ–°é—»æ­£æ–‡

    Returns:

    """
    # å¯¹æ–°é—»æ­£æ–‡è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶åˆ¤æ–­å¦‚æœè¶…é•¿åˆ™è¿›è¡Œæˆªæ–­
    if args.multi_prompt:
        maps = {}
        with open("data/claim_l2i_multi.json") as f:
            c2i = json.load(f)
            maps["muclaim2idx"] = c2i
            maps["idx2muclaim"] = {v: k for k, v in c2i.items()}
        claim_label2 = one_hot_labels([maps["muclaim2idx"][el] for el in claim_label2], maps["muclaim2idx"])
    content_tokens = tokenizer.tokenize(content)
    if len(content_tokens) > args.max_len - 3 - args.generate_max_len:
        content_tokens = content_tokens[:args.max_len - 3 - args.generate_max_len]
    # è·å–content_idã€title_idã€unk_idã€sep_idå€¼
    content_id = tokenizer.convert_tokens_to_ids("[Content]")
    title_id = tokenizer.convert_tokens_to_ids("[Title]")
    unk_id = tokenizer.convert_tokens_to_ids("[UNK]")
    sep_id = tokenizer.convert_tokens_to_ids("[SEP]")
    # å°†tokensç´¢å¼•åŒ–ï¼Œå˜æˆæ¨¡å‹æ‰€éœ€æ ¼å¼
    content_tokens = ["[CLS]"] + content_tokens + ["[SEP]"]
    input_ids = tokenizer.convert_tokens_to_ids(content_tokens)
    # å°†input_idså’Œtoken_type_idsè¿›è¡Œæ‰©å……ï¼Œæ‰©å……åˆ°éœ€è¦é¢„æµ‹æ ‡é¢˜çš„ä¸ªæ•°ï¼Œå³batch_size
    input_ids = [copy.deepcopy(input_ids) for _ in range(args.batch_size)]
    token_type_ids = [[content_id] * len(content_tokens) for _ in range(args.batch_size)]
    # å°†input_idså’Œtoken_type_idså˜æˆtensor
    input_tensors = torch.tensor(input_ids).long().to(device)
    token_type_tensors = torch.tensor(token_type_ids).long().to(device)

    next_token_type = torch.tensor([[title_id] for _ in range(args.batch_size)]).long().to(device)
    next_attention_mask = torch.tensor([[1] for _ in range(args.batch_size)]).long().to(device)
    # ç”¨äºå­˜æ”¾æ¯ä¸€æ­¥è§£ç çš„ç»“æœ
    generated = []
    # ç”¨äºå­˜æ”¾ï¼Œå®Œæˆè§£ç åºåˆ—çš„åºå·
    finish_set = set()

    input_ids, past_key_values_prompt, attention_mask, position_ids, token_type_ids = prompt_head(input_ids=input_tensors, \
                                                                    token_type_ids=token_type_tensors, claim_label2 = claim_label2)
    with torch.no_grad():
        # éå†ç”Ÿæˆæ ‡é¢˜æœ€å¤§é•¿åº¦
        for _ in range(args.generate_max_len):

            transformer_outputs = model.model.transformer(input_ids=input_ids,
                past_key_values = past_key_values_prompt,
                attention_mask=attention_mask,
                position_ids=position_ids, 
                token_type_ids=token_type_ids)

            hidden_states = transformer_outputs[0]
            lm_logits = model.model.lm_head(hidden_states)

            next_token_logits = lm_logits[:, -1, :]
            # å¯¹batch_sizeè¿›è¡Œéå†ï¼Œå°†è¯è¡¨ä¸­å‡ºç°åœ¨åºåˆ—ä¸­çš„è¯çš„æ¦‚ç‡è¿›è¡Œæƒ©ç½š
            for index in range(args.batch_size):
                for token_id in set([token_ids[index] for token_ids in generated]):
                    next_token_logits[index][token_id] /= args.repetition_penalty
            # å¯¹batch_sizeè¿›è¡Œéå†ï¼Œå°†è¯è¡¨ä¸­çš„UNKçš„å€¼è®¾ä¸ºæ— ç©·å°
            for next_token_logit in next_token_logits:
                next_token_logit[unk_id] = -float("Inf")
            # ä½¿ç”¨top_k_top_p_filteringå‡½æ•°ï¼ŒæŒ‰ç…§top_kå’Œtop_pçš„å€¼ï¼Œå¯¹é¢„æµ‹ç»“æœè¿›è¡Œç­›é€‰
            filter_logits = top_k_top_p_filtering(next_token_logits, top_k=args.top_k, top_p=args.top_p)
            # å¯¹filter_logitsçš„æ¯ä¸€è¡Œåšä¸€æ¬¡å–å€¼ï¼Œè¾“å‡ºç»“æœæ˜¯æ¯ä¸€æ¬¡å–å€¼æ—¶filter_logitså¯¹åº”è¡Œçš„ä¸‹æ ‡ï¼Œå³è¯è¡¨ä½ç½®ï¼ˆè¯çš„idï¼‰
            # filter_logitsä¸­çš„è¶Šå¤§çš„å€¼ï¼Œè¶Šå®¹æ˜“è¢«é€‰ä¸­
            next_tokens = torch.multinomial(F.softmax(filter_logits, dim=-1), num_samples=1)
            # åˆ¤æ–­å¦‚æœå“ªä¸ªåºåˆ—çš„é¢„æµ‹æ ‡è®°ä¸ºsep_idæ—¶ï¼Œåˆ™åŠ å…¥åˆ°finish_set
            for index, token_id in enumerate(next_tokens[:, 0]):
                if token_id == sep_id:
                    finish_set.add(index)
            # åˆ¤æ–­ï¼Œå¦‚æœfinish_setåŒ…å«å…¨éƒ¨çš„åºåˆ—åºå·ï¼Œåˆ™åœæ­¢é¢„æµ‹ï¼›å¦åˆ™ç»§ç»­é¢„æµ‹
            finish_flag = True
            for index in range(args.batch_size):
                if index not in finish_set:
                    finish_flag = False
                    break
            if finish_flag:
                break
            # å°†é¢„æµ‹æ ‡è®°æ·»åŠ åˆ°generatedä¸­
            generated.append([token.item() for token in next_tokens[:, 0]])
            # å°†é¢„æµ‹ç»“æœæ‹¼æ¥åˆ°input_tensorså’Œtoken_type_tensorsä¸Šï¼Œç»§ç»­ä¸‹ä¸€æ¬¡é¢„æµ‹
            
            input_ids = torch.cat([input_ids, next_tokens], dim=1)
            attention_mask = torch.cat((attention_mask, next_attention_mask), dim=-1)
            next_pos_ids = torch.tensor([[input_ids.shape[1]]]).long().to(device)
            position_ids = torch.cat((position_ids, next_pos_ids), dim=-1)
            token_type_ids = torch.cat((token_type_ids, next_token_type), dim=-1)
        # ç”¨äºå­˜å‚¨é¢„æµ‹ç»“æœ
        candidate_responses = []
        # å¯¹batch_sizeè¿›è¡Œéå†ï¼Œå¹¶å°†token_idå˜æˆå¯¹åº”æ±‰å­—
        for index in range(args.batch_size):
            responses = []
            for token_index in range(len(generated)):
                # åˆ¤æ–­ï¼Œå½“å‡ºç°sep_idæ—¶ï¼Œåœæ­¢åœ¨è¯¥åºåˆ—ä¸­æ·»åŠ token
                if generated[token_index][index] != sep_id:
                    responses.append(generated[token_index][index])
                else:
                    break
            # å°†token_idåºåˆ—å˜æˆæ±‰å­—åºåˆ—ï¼Œå»é™¤"##"ï¼Œå¹¶å°†[Space]æ›¿æ¢æˆç©ºæ ¼
            candidate_responses.append(
                "".join(tokenizer.convert_ids_to_tokens(responses)).replace("##", "").replace("[space]", " "))
    return candidate_responses

def calculate(pred, trg, filename):
    rouge = Rouge()
    bleuscore1, bleuscore2, bleuscoren = 0,0,0
    rougescore1, rougescore2, rougescorel = 0,0,0
    num = len(pred)
    for i in range(num):
        trg[i]=" ".join([w for w in trg[i].replace(" ","")])
        pred[i]=" ".join([w for w in pred[i].replace(" ","")])
        reference = [trg[i]]
        candidate = pred[i]
        rouge_score = rouge.get_scores(pred[i], trg[i])
        r1 = rouge_score[0]["rouge-1"]['r']
        r2 = rouge_score[0]["rouge-2"]['r']
        rl = rouge_score[0]["rouge-l"]['r']
        b1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))
        b2 = sentence_bleu(reference, candidate,weights=(0, 1, 0, 0))
        bn = sentence_bleu(reference, candidate,weights=(0.25, 0.25, 0.25, 0.25))
        rougescore1 += r1
        rougescore2 += r2
        rougescorel += rl
        bleuscore1 += b1
        bleuscore2 += b2
        bleuscoren += bn
        with open(filename,"a") as f:
            f.write(str(b1)+"\t"+ str(b2) +"\t"+str(bn) +"\t"+str(r1) +"\t"+str(r2) +"\t"+str(rl)+"\n")
    bleuscore1, bleuscore2, bleuscoren =  bleuscore1/num, bleuscore2/num, bleuscoren/num
    rougescore1, rougescore2, rougescorel = rougescore1/num, rougescore2/num, rougescorel/num
    return {"b1": bleuscore1, "b2": bleuscore2, "bn": bleuscoren,
            "r1": rougescore1, "r2": rougescore2, "rl": rougescorel}

def save_decode(pred, trg, filename=None):
    with open(filename,"a") as f:
        for i in range(len(pred)):
            f.write(str(trg[i]).replace(" ","").replace("\n","")+"\t"+\
            str(pred[i]).replace(" ","").replace("\n","").replace(" ","")+"\n")

RANDOM_SEED = 2022

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®é¢„æµ‹çš„é…ç½®å‚æ•°
    setup_seed(RANDOM_SEED)
    args = set_args()
    # è·å–è®¾å¤‡ä¿¡æ¯
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICE"] = args.device
    # è·å–deviceä¿¡æ¯ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ
    device = torch.device("cuda:0" if torch.cuda.is_available() and int(args.device) >= 0 else "cpu")
    args.device = device
    # å®ä¾‹åŒ–tokenizerå’Œmodel
    tokenizer = BertTokenizer.from_pretrained(args.vocab_path, do_lower_case=True)

    model = Distill_Tuning(args).to(device)
    model.prompt_encoder.load_state_dict(torch.load(args.prompt_model_path))
    model.eval()


    prompt_head = CaPromptHead(args, model.model, model.prompt_encoder)

    print('å¼€å§‹å¯¹æ–°é—»ç”Ÿæˆæ ‡é¢˜ï¼Œè¾“å…¥CTRL + Zï¼Œåˆ™é€€å‡º')
    source = []
    with open(args.test_file_path,'r',encoding="utf-8") as f:
        for line in f:
            source.append((json.loads(line)))
    df=pd.DataFrame(source)
    pred_all = []
    num = args.test_num if args.test_num!=-1 else len(df) #-1ç”¨æ‰€æœ‰æ•°æ®
    for i in tqdm(range(num)):
        tmp=""
        pos=[]
        cur_len=0
        for el in df["claim"][i]:
            tmp+=el
            cur_len+=len(el)
            pos.append(cur_len)
        df["claim"][i]=tmp
        df["fact"][i] = df["fact"][i] + df["claim"][i] 
        if args.multi_prompt:
            titles = predict_one_sample(model, prompt_head, tokenizer, device, args, df["fact"][i].replace("\n","").replace(" ","").replace("xx",""), df["claim_label2"][i])
        else:
            titles = predict_one_sample(model, prompt_head, tokenizer, device, args, df["fact"][i].replace("\n","").replace(" ","").replace("xx",""))
        pred_all.append(titles[0])
    metric_sum_lst = calculate(pred_all, df["view"][:num], args.metric_filename)
    save_decode(pred_all, df["view"][:num], args.txt_filename)
    pred_ans = cal_ans(pred_all, df["claim_label2"][:num], args)
    print("*"*10+"bleu"+"*"*10)
    print("bleu1:{0:.4f},bleu2:{1:.4f},bleun:{2:.4f}".format(
        metric_sum_lst["b1"], metric_sum_lst["b2"], metric_sum_lst["bn"]))
    print("*"*10+"rouge"+"*"*10)
    print("rouge1:{0:.4f},rouge2:{1:.4f},rougel:{2:.4f}".format(
        metric_sum_lst["r1"], metric_sum_lst["r2"], metric_sum_lst["rl"]))
    print("*"*10+"ans"+"*"*10)
    print("pred_ans:{0:.4f}".format(pred_ans)) 


if __name__ == '__main__':
    main()

